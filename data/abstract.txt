Pragmatic influences on the kinematic form of head nods across language modalities: Insights from a study of spoken and signed interactions
Anastasia Bauer1, Anna Kuder1, Marc Schulder2, Job Schepens1
1	Department of Linguistics, General Linguistics, University of Cologne, Cologne, Germany
2	Institute for German Sign Language and Communication of the Deaf, University of Hamburg, Hamburg, Germany
anastasia.bauer@uni-koeln.de
Head nods are among the most commonly produced bodily communication signals, characterized by up-and-down head movements, often repeated. Both signers/speakers, as well as addressees, produce head nods. A head nod is associated with a number of communicative functions, including affirmation, emphasis, evidentiality, or feedback [1]. However, most claims about the kinematic properties of head nods are based on manual annotations without reference to naturalistic data types, such as velocity (?). In addition, head nods produced by addressees have been largely overlooked (with notable exceptions by [2], [3]). Consequently, we lack detailed information about the kinematic properties of addressees’ head nods and the extent to which their linguistic functions influence their phonetic forms.
This study investigates the phonetic properties of addressees’ head nods in natural dyadic signed and spoken interactions. Building on [4], we expand the analysis to a larger, cross-modal sample of languages. Specifically, we examine whether head nods serving distinct pragmatic functions exhibit systematic variations in their phonetic characteristics.
We hypothesize that objective kinematic properties can reliably distinguish affirmation nods from feedback nods across both language modalities. We define affirmation as a positive response to a preceding question (i.e. it must be positive and it must follow a question), and a feedback nod as an interactional response that signals interlocutors’ any kind of perception or understanding of the ongoing conversation. We focus on a common feedback mechanism — continuers — which traditionally indicate the non-uptake of a conver- sational turn [5], [6].
We combine manual annotation in ELAN with quantitative analysis of body pose information generated using the computer vision toolkit OpenPose [7] to extract kinetic head nod measurements from video recordings and examine head nods in terms of their du- ration, amplitude and velocity (see Figure 1). The applicability of computer vision tools for phonetic analysis of non-manuals has been successfully tested for sign languages [8]. This study provides the first cross-linguistic and cross-modal analysis of head nods in in- teraction.
Our data was taken from comparable settings across all four languages that we investigated. All data were derived from free, spontaneous dyadic interactions without prompts. For German Sign Language (DGS), we utilized publicly available data from the DGS Corpus [9] For spoken German, we collected multimodal data from natural conversations in a controlled lab setting. Data for Russian Sign Language (RSL) were sourced from a corpus of RSL conversations [10], comprising video recordings of casual dyadic inter- actions.. These recordings feature Deaf native RSL signers residing in Germany, most of whom were well-acquainted with XXX. Conversations covered diverse topics, includ- ing experiences in Russia before immigration and the lives of Deaf individuals in Europe and Russia. For spoken Russian, we analyzed data from a multimodal corpus of spoken Russian di- alogues [10]. This corpus consists of seven dialogues among Russian immigrants in Germany,. The participants, aged 20 to 30, are native Russian speakers who have resided in Germany for no longer than five years. We investigated ca. 2 - 4 hours of naturalistic dyadic interaction per language and identified more than 1000 occurrences of nods in each dataset. 

The results show that phonetic properties of affirmative nods differ from those of feedback nods in velocity and maximal amplitude. Feedback nods appear to be on average slower in production and smaller in amplitude than affirmation nods.
We attribute the variation in phonetic properties of head nods to the distinct roles these cues fulfill in the turn-taking system: feedback nods typically signal passive recipiency, while affirmation nods result in a more prominent visual signal, which is easier to detect and which displays the potential for interrupting the conversational flow.
Our findings suggest that the immediate pragmatic function (e.g., affirmation or feedback) strongly influences the physical form of non-manual gestures (see [11] for a similar finding with regard to manual gestures). Results reveal no significant cross-modal differences in the phonetic properties of head nods, suggesting a shared cognitive mechanism underpinning their use in interaction. The functional differentiation of head nod forms appears to enhance communicative efficiency, supporting the smooth exchange of information in interaction.

Figure 1: Visualization of head nods. The source video (left) is overlaid with the OpenPose body points used for the calculations. On the line graph (upper right) the upper (blue) line represents the vertical motion of the nose relative to body position, while the lower (red) line indicates the nose location prediction confidence of OpenPose (worse during blur or when occluded). Light blue boxes indicate durations manually labeled as head nods. The spectrogram (lower right) visualizes the spectrum of frequencies of vertical nose movement, with brighter areas indicating repeated up and down motion as during nodding.


References
[1]	L. Cerrato, “Linguistic functions of head nods,” in Proceedings from The Second Nordic Conference on Multimodal Communication, Gothenburg, Sweden: Göteborg University, 2005, pp. 137–152.
[2]	A. Puupponen, T. Wainio, B. Burger, and T. Jantunen, “Head movements in finnish sign language on the basis of motion capture data: A study of the form and function of nods, nodding, head thrusts, and head pulls,” Sign Language & Linguistics, vol. 18, no. 1,
pp. 41–89, 2015. DOI: 10.1075/sll.18.1.02puu.
[3]	J. Mesch, “Manual backchannel responses in signers’ conversations in swedish sign lan- guage,” Language & Communication, vol. 50, pp. 22–41, 2016. DOI: 10 . 1016 / j . langcom.2016.08.011.
[4]	A. Bauer, A. Kuder, M. Schulder, and J. Schepens, “Phonetic differences between af- firmative and feedback head nods in german sign language (DGS): A pose estimation study,” PLOS ONE, vol. 19, no. 5, L. Morett, Ed., e0304040, 2024. DOI: 10 . 1371 /
journal.pone.0304040.
 
[5]	R. Gardner, When Listeners Talk: Response tokens and listener stance (Pragmatics & Be- yond New Series 92). Amsterdam: John Benjamins Publishing Company, 2001, 289 pp.
DOI: 10.1075/pbns.92.
[6]	M. Dingemanse, A. Liesenfeld, and M. Woensdregt, “Convergent cultural evolution of continuers (mmhm),” PsyArXiv, preprint, Sep. 6, 2022. DOI: 10 . 31234 / osf . io / 65c79. [Online]. Available: https://osf.io/65c79 (visited on 08/15/2023).
[7]	Z. Cao, G. Hidalgo Martinez, T. Simon, S.-E. Wei, and Y. A. Sheikh, “OpenPose: Re- altime multi-person 2d pose estimation using part affinity fields,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 1, pp. 172–186, 2021. DOI: 10.1109/TPAMI.2019.2929257.
[8]	A. Chizhikova and V. Kimmelman, “Phonetics of negative headshake in russian sign language: A small-scale corpus study,” in Proceedings of the LREC2022 10th Workshop on the Representation and Processing of Sign Languages: Multilingual Sign Language Resources, E. Efthimiou, S.-E. Fotinea, T. Hanke, et al., Eds., European Language Re- sources Association (ELRA), 2022, pp. 29–36.
[9]	R. Konrad, T. Hanke, G. Langer, et al., MY DGS – annotated. public corpus of german sign language, 3rd release, version 3.0, 2020. DOI: 10.25592/dgs.corpus-3.0.
[10]	A. Bauer, Russian multimodal conversational data, version 1.0, Artwork Size: 44 GB Pages: 44 GB, 2023. DOI: 10.18716/DCH/A.00000016.
[11]	J. Gerwing and J. Bavelas, “Linguistic influences on gesture’s form,” Gesture, vol. 4, no. 2, pp. 157–195, Feb. 11, 2005. DOI: 10.1075/gest.4.2.04ger.
